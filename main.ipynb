{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876b315e",
   "metadata": {},
   "source": [
    "## Aircraft Predictive Maintenance\n",
    "\n",
    "Dataset:\n",
    "- https://www.kaggle.com/datasets/maternusherold/pred-maintanance-data?resource=download\n",
    "- Copied to /dataset\n",
    "\n",
    "Include:\n",
    "- Data preparation and data cleaning\n",
    "- EDA, feature importance analysis\n",
    "- Model selection process and parameter tuning\n",
    "\n",
    "## Problem Overview\n",
    "Predict the Remaining Useful Life (RUL) of aircraft engines using sensor data. This is a time-series regression problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa9828",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffec71d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define data directory and column names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m DATA_DIR = \u001b[43mPath\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Column names: engine_id, cycle, and 21 sensors\u001b[39;00m\n\u001b[32m      5\u001b[39m column_names = [\u001b[33m'\u001b[39m\u001b[33mengine_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcycle\u001b[39m\u001b[33m'\u001b[39m] + [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33msensor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m22\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# Define data directory and column names\n",
    "DATA_DIR = Path('dataset')\n",
    "\n",
    "# Column names: engine_id, cycle, and 21 sensors\n",
    "column_names = ['engine_id', 'cycle'] + [f'sensor_{i:02d}' for i in range(1, 22)]\n",
    "\n",
    "# Load training data\n",
    "# Note: Using sep=r'\\s+' to handle multiple spaces correctly\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(\n",
    "    DATA_DIR / 'PM_train.txt',\n",
    "    sep=r'\\s+',\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    usecols=range(23)  # Only use first 23 columns\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Number of engines: {train_df['engine_id'].nunique()}\")\n",
    "print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c7137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(\n",
    "    DATA_DIR / 'PM_test.txt',\n",
    "    sep=r'\\s+',\n",
    "    header=None,\n",
    "    names=column_names,\n",
    "    usecols=range(23)  # Only use first 23 columns\n",
    ")\n",
    "\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Number of engines: {test_df['engine_id'].nunique()}\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth RUL values for test set\n",
    "print(\"Loading ground truth RUL values...\")\n",
    "truth_df = pd.read_csv(\n",
    "    DATA_DIR / 'PM_truth.txt',\n",
    "    header=None,\n",
    "    names=['RUL']\n",
    ")\n",
    "\n",
    "print(f\"Ground truth shape: {truth_df.shape}\")\n",
    "truth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774480ae",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1062b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about training data\n",
    "print(\"=== Training Data Info ===\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"\\nNumber of unique engines: {train_df['engine_id'].nunique()}\")\n",
    "print(f\"Engine IDs range: {train_df['engine_id'].min()} to {train_df['engine_id'].max()}\")\n",
    "print(f\"\\nCycle range: {train_df['cycle'].min()} to {train_df['cycle'].max()}\")\n",
    "print(f\"\\nData types:\\n{train_df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{train_df.isnull().sum().sum()} total missing values\")\n",
    "print(f\"\\nMissing values per column:\\n{train_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f6b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cycles per engine\n",
    "cycles_per_engine = train_df.groupby('engine_id')['cycle'].max()\n",
    "print(\"=== Cycles per Engine ===\")\n",
    "print(f\"Min cycles: {cycles_per_engine.min()}\")\n",
    "print(f\"Max cycles: {cycles_per_engine.max()}\")\n",
    "print(f\"Mean cycles: {cycles_per_engine.mean():.2f}\")\n",
    "print(f\"Median cycles: {cycles_per_engine.median():.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(cycles_per_engine, bins=50, edgecolor='black')\n",
    "plt.xlabel('Max Cycles per Engine')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Maximum Cycles per Engine')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7258225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for sensors\n",
    "print(\"=== Sensor Statistics ===\")\n",
    "sensor_cols = [f'sensor_{i:02d}' for i in range(1, 22)]\n",
    "train_df[sensor_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6db26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for constant or low-variance sensors\n",
    "print(\"=== Sensor Variance Analysis ===\")\n",
    "sensor_variance = train_df[sensor_cols].var().sort_values()\n",
    "print(\"Sensors with lowest variance:\")\n",
    "print(sensor_variance.head(10))\n",
    "print(\"\\nSensors with highest variance:\")\n",
    "print(sensor_variance.tail(10))\n",
    "\n",
    "# Identify constant sensors (variance = 0 or very close to 0)\n",
    "constant_sensors = sensor_variance[sensor_variance < 1e-10]\n",
    "if len(constant_sensors) > 0:\n",
    "    print(f\"\\nConstant sensors (variance < 1e-10): {constant_sensors.index.tolist()}\")\n",
    "else:\n",
    "    print(\"\\nNo constant sensors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949dd07",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0614f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "train_clean = train_df.copy()\n",
    "test_clean = test_df.copy()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"=== Missing Values Check ===\")\n",
    "print(f\"Training missing values: {train_clean.isnull().sum().sum()}\")\n",
    "print(f\"Test missing values: {test_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "print(f\"\\nTraining infinite values: {np.isinf(train_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Test infinite values: {np.isinf(test_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# If there are missing values, we'll handle them (forward fill for time-series)\n",
    "if train_clean.isnull().sum().sum() > 0:\n",
    "    train_clean = train_clean.fillna(method='ffill').fillna(method='bfill')\n",
    "    print(\"\\nFilled missing values using forward fill and backward fill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify constant sensors (sensors with zero or very low variance)\n",
    "# These sensors don't provide useful information for prediction\n",
    "sensor_variance = train_clean[sensor_cols].var()\n",
    "constant_threshold = 1e-10\n",
    "constant_sensors = sensor_variance[sensor_variance < constant_threshold].index.tolist()\n",
    "\n",
    "print(f\"=== Constant Sensors (variance < {constant_threshold}) ===\")\n",
    "if constant_sensors:\n",
    "    print(f\"Found {len(constant_sensors)} constant sensors: {constant_sensors}\")\n",
    "    print(\"These will be excluded from feature engineering\")\n",
    "else:\n",
    "    print(\"No constant sensors found\")\n",
    "\n",
    "# Store for later use\n",
    "sensors_to_exclude = constant_sensors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6099d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RUL for training data\n",
    "# RUL = max_cycles_per_engine - current_cycle\n",
    "print(\"=== Calculating RUL ===\")\n",
    "\n",
    "# Get maximum cycle for each engine\n",
    "max_cycles = train_clean.groupby('engine_id')['cycle'].max().reset_index()\n",
    "max_cycles.columns = ['engine_id', 'max_cycle']\n",
    "\n",
    "# Merge with training data\n",
    "train_clean = train_clean.merge(max_cycles, on='engine_id', how='left')\n",
    "\n",
    "# Calculate RUL\n",
    "train_clean['RUL'] = train_clean['max_cycle'] - train_clean['cycle']\n",
    "\n",
    "print(f\"RUL range: {train_clean['RUL'].min()} to {train_clean['RUL'].max()}\")\n",
    "print(f\"Mean RUL: {train_clean['RUL'].mean():.2f}\")\n",
    "print(f\"Median RUL: {train_clean['RUL'].median():.2f}\")\n",
    "\n",
    "# Apply piecewise linear degradation (cap RUL at 125 cycles)\n",
    "# This is a common approach in predictive maintenance\n",
    "RUL_CAP = 125\n",
    "train_clean['RUL_capped'] = train_clean['RUL'].clip(upper=RUL_CAP)\n",
    "\n",
    "print(f\"\\nAfter capping at {RUL_CAP} cycles:\")\n",
    "print(f\"RUL_capped range: {train_clean['RUL_capped'].min()} to {train_clean['RUL_capped'].max()}\")\n",
    "print(f\"Mean RUL_capped: {train_clean['RUL_capped'].mean():.2f}\")\n",
    "\n",
    "# Use capped RUL as target\n",
    "train_clean['target'] = train_clean['RUL_capped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f839dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RUL distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original RUL\n",
    "axes[0].hist(train_clean['RUL'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(RUL_CAP, color='r', linestyle='--', linewidth=2, label=f'Cap at {RUL_CAP}')\n",
    "axes[0].set_xlabel('RUL (cycles)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Original RUL Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Capped RUL\n",
    "axes[1].hist(train_clean['RUL_capped'], bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_xlabel('RUL (cycles)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Capped RUL Distribution (max {RUL_CAP})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation: Check for data leakage and temporal ordering\n",
    "print(\"=== Data Validation ===\")\n",
    "\n",
    "# Check temporal ordering (cycles should be increasing for each engine)\n",
    "for engine_id in train_clean['engine_id'].unique()[:5]:  # Check first 5 engines\n",
    "    engine_data = train_clean[train_clean['engine_id'] == engine_id].sort_values('cycle')\n",
    "    cycles = engine_data['cycle'].values\n",
    "    is_sorted = np.all(cycles[:-1] <= cycles[1:])\n",
    "    if not is_sorted:\n",
    "        print(f\"WARNING: Engine {engine_id} cycles are not sorted!\")\n",
    "    else:\n",
    "        print(f\"Engine {engine_id}: Cycles are properly ordered\")\n",
    "\n",
    "# Check RUL calculation\n",
    "# RUL should decrease as cycle increases\n",
    "sample_engine = train_clean[train_clean['engine_id'] == 1].sort_values('cycle')\n",
    "print(f\"\\nSample engine 1: Cycle {sample_engine['cycle'].iloc[0]} -> RUL {sample_engine['RUL'].iloc[0]}\")\n",
    "print(f\"Sample engine 1: Cycle {sample_engine['cycle'].iloc[-1]} -> RUL {sample_engine['RUL'].iloc[-1]}\")\n",
    "print(f\"RUL decreases: {sample_engine['RUL'].iloc[0] > sample_engine['RUL'].iloc[-1]}\")\n",
    "\n",
    "print(\"\\nData validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543106a4",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39f48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def create_features(df, sensors_to_exclude=None):\n",
    "    \"\"\"\n",
    "    Create engineered features from raw sensor data.\n",
    "    \n",
    "    Features include:\n",
    "    - Time-based features\n",
    "    - Rolling statistics\n",
    "    - Degradation indicators\n",
    "    - Engine-level aggregations\n",
    "    - Sensor interactions\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Get sensor columns (exclude constant sensors)\n",
    "    if sensors_to_exclude is None:\n",
    "        sensors_to_exclude = []\n",
    "    sensor_cols = [col for col in df.columns if col.startswith('sensor_') and col not in sensors_to_exclude]\n",
    "    \n",
    "    # Sort by engine_id and cycle to ensure proper ordering\n",
    "    df = df.sort_values(['engine_id', 'cycle']).reset_index(drop=True)\n",
    "    \n",
    "    # === Time-based features ===\n",
    "    df['cycle_norm'] = df.groupby('engine_id')['cycle'].transform(lambda x: (x - x.min()) / (x.max() - x.min() + 1))\n",
    "    \n",
    "    # === Rolling statistics (window = 5 cycles) ===\n",
    "    window = 5\n",
    "    for sensor in sensor_cols:\n",
    "        # Rolling mean\n",
    "        df[f'{sensor}_rolling_mean'] = df.groupby('engine_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        # Rolling std\n",
    "        df[f'{sensor}_rolling_std'] = df.groupby('engine_id')[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std().fillna(0)\n",
    "        )\n",
    "    \n",
    "    # === Degradation indicators (rate of change) ===\n",
    "    for sensor in sensor_cols:\n",
    "        # Rate of change (difference from previous cycle)\n",
    "        df[f'{sensor}_diff'] = df.groupby('engine_id')[sensor].diff().fillna(0)\n",
    "        # Rate of change normalized by cycle\n",
    "        df[f'{sensor}_diff_norm'] = df[f'{sensor}_diff'] / (df['cycle'] + 1)\n",
    "    \n",
    "    # === Engine-level aggregations ===\n",
    "    engine_stats = df.groupby('engine_id')[sensor_cols].agg(['max', 'min', 'mean', 'std']).reset_index()\n",
    "    engine_stats.columns = ['engine_id'] + [f'{col}_{stat}' for col in sensor_cols for stat in ['max', 'min', 'mean', 'std']]\n",
    "    \n",
    "    # Merge engine-level stats\n",
    "    df = df.merge(engine_stats, on='engine_id', how='left')\n",
    "    \n",
    "    # === Sensor interactions (ratios between related sensors) ===\n",
    "    # Create ratios for sensors that might be related (e.g., temperature/pressure ratios)\n",
    "    # We'll create a few key ratios based on sensor indices\n",
    "    if len(sensor_cols) >= 4:\n",
    "        # Ratio of sensor 1 to sensor 2 (if they exist)\n",
    "        if f'sensor_01' in sensor_cols and f'sensor_02' in sensor_cols:\n",
    "            df['sensor_ratio_01_02'] = df['sensor_01'] / (df['sensor_02'] + 1e-10)\n",
    "        # Ratio of sensor 3 to sensor 4\n",
    "        if f'sensor_03' in sensor_cols and f'sensor_04' in sensor_cols:\n",
    "            df['sensor_ratio_03_04'] = df['sensor_03'] / (df['sensor_04'] + 1e-10)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Feature engineering function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4aadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to training data\n",
    "print(\"Creating features for training data...\")\n",
    "train_features = create_features(train_clean, sensors_to_exclude=sensors_to_exclude)\n",
    "\n",
    "print(f\"Original columns: {len(train_clean.columns)}\")\n",
    "print(f\"After feature engineering: {len(train_features.columns)}\")\n",
    "print(f\"New features created: {len(train_features.columns) - len(train_clean.columns)}\")\n",
    "\n",
    "# Display feature columns\n",
    "feature_cols = [col for col in train_features.columns \n",
    "                if col not in ['engine_id', 'cycle', 'max_cycle', 'RUL', 'RUL_capped', 'target'] \n",
    "                and not col.startswith('sensor_')]\n",
    "print(f\"\\nEngineered feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols[:20])  # Show first 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix for modeling\n",
    "# Select all features except metadata columns\n",
    "exclude_cols = ['engine_id', 'cycle', 'max_cycle', 'RUL', 'RUL_capped', 'target']\n",
    "feature_cols_all = [col for col in train_features.columns if col not in exclude_cols]\n",
    "\n",
    "# Also exclude original sensor columns if we want to use only engineered features\n",
    "# Or keep them - we'll let the model decide\n",
    "# For now, keep all features\n",
    "X_train = train_features[feature_cols_all].copy()\n",
    "y_train = train_features['target'].copy()\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n",
    "print(f\"\\nFeature columns: {len(feature_cols_all)}\")\n",
    "print(f\"Sample features: {feature_cols_all[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining issues in features\n",
    "print(\"=== Feature Quality Check ===\")\n",
    "print(f\"Missing values in features: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Infinite values in features: {np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Fill any remaining NaN values (shouldn't be any, but just in case)\n",
    "if X_train.isnull().sum().sum() > 0:\n",
    "    X_train = X_train.fillna(0)\n",
    "    print(\"Filled NaN values with 0\")\n",
    "\n",
    "# Replace infinite values\n",
    "if np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum() > 0:\n",
    "    X_train = X_train.replace([np.inf, -np.inf], 0)\n",
    "    print(\"Replaced infinite values with 0\")\n",
    "\n",
    "print(\"Feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9acb9b1",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4123ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis: RUL Distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RUL histogram\n",
    "axes[0].hist(y_train, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('RUL (cycles)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Variable (RUL) Distribution')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RUL box plot\n",
    "axes[1].boxplot(y_train, vert=True)\n",
    "axes[1].set_ylabel('RUL (cycles)')\n",
    "axes[1].set_title('RUL Box Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"RUL Statistics:\")\n",
    "print(f\"Mean: {y_train.mean():.2f}\")\n",
    "print(f\"Median: {y_train.median():.2f}\")\n",
    "print(f\"Std: {y_train.std():.2f}\")\n",
    "print(f\"Min: {y_train.min():.2f}\")\n",
    "print(f\"Max: {y_train.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis: Sample sensor distributions\n",
    "# Plot distributions of a few key sensors\n",
    "sample_sensors = ['sensor_01', 'sensor_02', 'sensor_03', 'sensor_04']\n",
    "if all(col in X_train.columns for col in sample_sensors):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, sensor in enumerate(sample_sensors):\n",
    "        axes[idx].hist(X_train[sensor], bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_xlabel(sensor)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].set_title(f'{sensor} Distribution')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis: Correlation matrix for original sensors\n",
    "original_sensors = [col for col in X_train.columns if col.startswith('sensor_') and not any(x in col for x in ['rolling', 'diff', 'ratio', '_max', '_min', '_mean', '_std'])]\n",
    "\n",
    "if len(original_sensors) > 0:\n",
    "    # Sample a subset for visualization (too many sensors)\n",
    "    sensor_sample = original_sensors[:10] if len(original_sensors) >= 10 else original_sensors\n",
    "    \n",
    "    corr_matrix = X_train[sensor_sample].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Matrix: Original Sensors (Sample)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59416b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series Analysis: Sensor degradation patterns\n",
    "# Visualize how sensors change over cycles for a few sample engines\n",
    "sample_engines = [1, 2, 3]\n",
    "sample_sensors_viz = ['sensor_01', 'sensor_02', 'sensor_03']\n",
    "\n",
    "fig, axes = plt.subplots(len(sample_engines), 1, figsize=(14, 4*len(sample_engines)))\n",
    "\n",
    "for idx, engine_id in enumerate(sample_engines):\n",
    "    engine_data = train_features[train_features['engine_id'] == engine_id].sort_values('cycle')\n",
    "    \n",
    "    for sensor in sample_sensors_viz:\n",
    "        if sensor in engine_data.columns:\n",
    "            axes[idx].plot(engine_data['cycle'], engine_data[sensor], \n",
    "                          label=sensor, alpha=0.7, linewidth=2)\n",
    "    \n",
    "    axes[idx].set_xlabel('Cycle')\n",
    "    axes[idx].set_ylabel('Sensor Value')\n",
    "    axes[idx].set_title(f'Engine {engine_id}: Sensor Trends Over Time')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series Analysis: RUL vs Cycle for sample engines\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: RUL over cycles for sample engines\n",
    "sample_engines = [1, 2, 3, 4, 5]\n",
    "for engine_id in sample_engines:\n",
    "    engine_data = train_features[train_features['engine_id'] == engine_id].sort_values('cycle')\n",
    "    axes[0].plot(engine_data['cycle'], engine_data['RUL'], \n",
    "                label=f'Engine {engine_id}', alpha=0.7, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('Cycle')\n",
    "axes[0].set_ylabel('RUL (cycles)')\n",
    "axes[0].set_title('RUL Degradation Over Cycles (Sample Engines)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Average RUL by cycle across all engines\n",
    "avg_rul_by_cycle = train_features.groupby('cycle')['RUL'].mean()\n",
    "axes[1].plot(avg_rul_by_cycle.index, avg_rul_by_cycle.values, \n",
    "            linewidth=2, color='red', alpha=0.7)\n",
    "axes[1].set_xlabel('Cycle')\n",
    "axes[1].set_ylabel('Average RUL (cycles)')\n",
    "axes[1].set_title('Average RUL Across All Engines')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766ecce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: Correlation with RUL\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate correlation of each feature with RUL\n",
    "feature_correlations = []\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype in [np.float64, np.int64]:\n",
    "        try:\n",
    "            corr, p_value = pearsonr(X_train[col], y_train)\n",
    "            feature_correlations.append({\n",
    "                'feature': col,\n",
    "                'correlation': abs(corr),\n",
    "                'p_value': p_value\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Create DataFrame and sort by correlation\n",
    "corr_df = pd.DataFrame(feature_correlations)\n",
    "corr_df = corr_df.sort_values('correlation', ascending=False)\n",
    "\n",
    "# Display top 20 features by correlation\n",
    "print(\"=== Top 20 Features by Correlation with RUL ===\")\n",
    "print(corr_df.head(20))\n",
    "\n",
    "# Visualize top features\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = corr_df.head(top_n)\n",
    "plt.barh(range(len(top_features)), top_features['correlation'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Absolute Correlation with RUL')\n",
    "plt.title(f'Top {top_n} Features by Correlation with RUL')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7de344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: Mutual Information (from scikit-learn)\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Sample data for faster computation (mutual info can be slow on large datasets)\n",
    "sample_size = min(5000, len(X_train))\n",
    "sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_sample = X_train.iloc[sample_idx]\n",
    "y_sample = y_train.iloc[sample_idx]\n",
    "\n",
    "# Calculate mutual information\n",
    "print(\"Calculating mutual information scores (this may take a moment)...\")\n",
    "mi_scores = mutual_info_regression(X_sample, y_sample, random_state=42)\n",
    "\n",
    "# Create DataFrame\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'mutual_info': mi_scores\n",
    "}).sort_values('mutual_info', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"\\n=== Top 20 Features by Mutual Information ===\")\n",
    "print(mi_df.head(20))\n",
    "\n",
    "# Visualize top features\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features_mi = mi_df.head(top_n)\n",
    "plt.barh(range(len(top_features_mi)), top_features_mi['mutual_info'].values)\n",
    "plt.yticks(range(len(top_features_mi)), top_features_mi['feature'].values)\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.title(f'Top {top_n} Features by Mutual Information')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543be969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Combine correlation and mutual information for feature ranking\n",
    "feature_importance = corr_df.merge(mi_df, on='feature', how='outer')\n",
    "feature_importance['correlation'] = feature_importance['correlation'].fillna(0)\n",
    "feature_importance['mutual_info'] = feature_importance['mutual_info'].fillna(0)\n",
    "\n",
    "# Normalize scores (0-1 scale) and create combined score\n",
    "feature_importance['corr_norm'] = (feature_importance['correlation'] - feature_importance['correlation'].min()) / (feature_importance['correlation'].max() - feature_importance['correlation'].min() + 1e-10)\n",
    "feature_importance['mi_norm'] = (feature_importance['mutual_info'] - feature_importance['mutual_info'].min()) / (feature_importance['mutual_info'].max() - feature_importance['mutual_info'].min() + 1e-10)\n",
    "\n",
    "# Combined score (weighted average)\n",
    "feature_importance['combined_score'] = 0.5 * feature_importance['corr_norm'] + 0.5 * feature_importance['mi_norm']\n",
    "feature_importance = feature_importance.sort_values('combined_score', ascending=False)\n",
    "\n",
    "print(\"=== Top 20 Features by Combined Importance Score ===\")\n",
    "print(feature_importance[['feature', 'correlation', 'mutual_info', 'combined_score']].head(20))\n",
    "\n",
    "# Save feature importance for later use\n",
    "feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "print(\"\\nFeature importance saved to 'feature_importance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83f7cc",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6960e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split (engine-based split to avoid data leakage)\n",
    "# Use 80% of engines for training, 20% for validation\n",
    "print(\"=== Creating Train/Validation Split ===\")\n",
    "\n",
    "# Get engine IDs\n",
    "engine_ids = train_features['engine_id'].values\n",
    "\n",
    "# Split by engines, not by samples\n",
    "unique_engines = np.unique(engine_ids)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_engines)\n",
    "n_train_engines = int(len(unique_engines) * 0.8)\n",
    "train_engines = set(unique_engines[:n_train_engines])\n",
    "val_engines = set(unique_engines[n_train_engines:])\n",
    "\n",
    "train_mask = np.array([eid in train_engines for eid in engine_ids])\n",
    "val_mask = np.array([eid in val_engines for eid in engine_ids])\n",
    "\n",
    "X_train_split, y_train_split = X_train[train_mask], y_train[train_mask]\n",
    "X_val_split, y_val_split = X_train[val_mask], y_train[val_mask]\n",
    "\n",
    "print(f\"Train engines: {len(train_engines)}\")\n",
    "print(f\"Validation engines: {len(val_engines)}\")\n",
    "print(f\"Train samples: {X_train_split.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val_split.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1258017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    # Training predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Validation predictions\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"  Train - MAE: {train_mae:.4f}, RMSE: {train_rmse:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"  Val   - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_mae': train_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'val_mae': val_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514ea88",
   "metadata": {},
   "source": [
    "### 6.1 Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training Baseline Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "# 1. Linear Regression\n",
    "print(\"\\n1. Linear Regression\")\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_split, y_train_split)\n",
    "baseline_results.append(evaluate_model(lr, X_train_split, y_train_split, X_val_split, y_val_split, \"Linear Regression\"))\n",
    "\n",
    "# 2. Ridge Regression\n",
    "print(\"\\n2. Ridge Regression\")\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_split, y_train_split)\n",
    "baseline_results.append(evaluate_model(ridge, X_train_split, y_train_split, X_val_split, y_val_split, \"Ridge Regression\"))\n",
    "\n",
    "# 3. Random Forest\n",
    "print(\"\\n3. Random Forest\")\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_split, y_train_split)\n",
    "baseline_results.append(evaluate_model(rf, X_train_split, y_train_split, X_val_split, y_val_split, \"Random Forest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22041c7",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65b6aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization functions\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def optimize_xgboost(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Optimize XGBoost hyperparameters.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params, n_jobs=-1)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "\n",
    "def optimize_lightgbm(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Optimize LightGBM hyperparameters.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params, n_jobs=-1, verbose=-1)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "print(\"Optimization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize XGBoost\n",
    "print(\"=\"*60)\n",
    "print(\"Optimizing XGBoost\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "study_xgb.optimize(\n",
    "    lambda trial: optimize_xgboost(trial, X_train_split, y_train_split, X_val_split, y_val_split),\n",
    "    n_trials=30,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest XGBoost MAE: {study_xgb.best_value:.4f}\")\n",
    "print(f\"Best XGBoost params: {study_xgb.best_params}\")\n",
    "\n",
    "# Train best XGBoost model\n",
    "xgb_best = xgb.XGBRegressor(**study_xgb.best_params, objective='reg:squarederror', \n",
    "                             eval_metric='rmse', tree_method='hist', random_state=42, n_jobs=-1)\n",
    "xgb_best.fit(X_train_split, y_train_split, eval_set=[(X_val_split, y_val_split)], verbose=False)\n",
    "xgb_result = evaluate_model(xgb_best, X_train_split, y_train_split, X_val_split, y_val_split, \"XGBoost (Optimized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a179d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize LightGBM\n",
    "print(\"=\"*60)\n",
    "print(\"Optimizing LightGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "study_lgb = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "study_lgb.optimize(\n",
    "    lambda trial: optimize_lightgbm(trial, X_train_split, y_train_split, X_val_split, y_val_split),\n",
    "    n_trials=30,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nBest LightGBM MAE: {study_lgb.best_value:.4f}\")\n",
    "print(f\"Best LightGBM params: {study_lgb.best_params}\")\n",
    "\n",
    "# Train best LightGBM model\n",
    "lgb_best = lgb.LGBMRegressor(**study_lgb.best_params, objective='regression', \n",
    "                              metric='mae', boosting_type='gbdt', random_state=42, \n",
    "                              n_jobs=-1, verbose=-1)\n",
    "lgb_best.fit(X_train_split, y_train_split, eval_set=[(X_val_split, y_val_split)], \n",
    "             callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "lgb_result = evaluate_model(lgb_best, X_train_split, y_train_split, X_val_split, y_val_split, \"LightGBM (Optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3e55e",
   "metadata": {},
   "source": [
    "### 6.3 Model Comparison and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22623716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = baseline_results + [xgb_result, lgb_result]\n",
    "\n",
    "# Create results summary\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        'model': r['model_name'],\n",
    "        'train_mae': r['train_mae'],\n",
    "        'train_rmse': r['train_rmse'],\n",
    "        'train_r2': r['train_r2'],\n",
    "        'val_mae': r['val_mae'],\n",
    "        'val_rmse': r['val_rmse'],\n",
    "        'val_r2': r['val_r2']\n",
    "    }\n",
    "    for r in all_results\n",
    "]).sort_values('val_mae')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Comparison Summary\")\n",
    "print(\"=\"*60)\n",
    "print(results_summary)\n",
    "\n",
    "# Find best model\n",
    "best_result = min(all_results, key=lambda x: x['val_mae'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Best Model\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {best_result['model_name']}\")\n",
    "print(f\"Validation MAE: {best_result['val_mae']:.4f}\")\n",
    "print(f\"Validation RMSE: {best_result['val_rmse']:.4f}\")\n",
    "print(f\"Validation R²: {best_result['val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d58ac9",
   "metadata": {},
   "source": [
    "### 6.4 Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and metadata\n",
    "import joblib\n",
    "\n",
    "print(\"Saving model and metadata...\")\n",
    "\n",
    "# Save best model\n",
    "joblib.dump(best_result['model'], 'model.pkl')\n",
    "print(\"✓ Saved model.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_cols_all, 'feature_columns.pkl')\n",
    "print(\"✓ Saved feature_columns.pkl\")\n",
    "\n",
    "# Save sensors to exclude\n",
    "joblib.dump(sensors_to_exclude, 'sensors_to_exclude.pkl')\n",
    "print(\"✓ Saved sensors_to_exclude.pkl\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary.to_csv('model_results.csv', index=False)\n",
    "print(\"✓ Saved model_results.csv\")\n",
    "\n",
    "print(\"\\nAll model artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e04b0",
   "metadata": {},
   "source": [
    "### 6.5 LSTM Models (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM requires sequence data - we need to reshape our data\n",
    "# For time-series with LSTM, we'll use a sliding window approach\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequence data for LSTM\n",
    "def create_sequences(df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM from time-series data.\n",
    "    Each sequence contains the last N cycles for an engine.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by engine\n",
    "    for engine_id in df['engine_id'].unique():\n",
    "        engine_data = df[df['engine_id'] == engine_id].sort_values('cycle')\n",
    "        \n",
    "        # Get features and target\n",
    "        features = engine_data[feature_cols_all].values\n",
    "        target_values = engine_data['target'].values\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(features) - sequence_length + 1):\n",
    "            sequences.append(features[i:i+sequence_length])\n",
    "            targets.append(target_values[i+sequence_length-1])  # Predict for last cycle in sequence\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Use shorter sequences for faster training\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "print(f\"Creating sequences with length {SEQUENCE_LENGTH}...\")\n",
    "X_train_seq, y_train_seq = create_sequences(\n",
    "    train_features[train_mask], \n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "X_val_seq, y_val_seq = create_sequences(\n",
    "    train_features[val_mask], \n",
    "    sequence_length=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val_seq.shape}\")\n",
    "print(f\"Features per timestep: {X_train_seq.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c907d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features for LSTM\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape for scaling\n",
    "X_train_seq_reshaped = X_train_seq.reshape(-1, X_train_seq.shape[2])\n",
    "X_val_seq_reshaped = X_val_seq.reshape(-1, X_val_seq.shape[2])\n",
    "\n",
    "# Fit and transform\n",
    "X_train_seq_scaled = scaler.fit_transform(X_train_seq_reshaped)\n",
    "X_val_seq_scaled = scaler.transform(X_val_seq_reshaped)\n",
    "\n",
    "# Reshape back to sequences\n",
    "X_train_seq_scaled = X_train_seq_scaled.reshape(X_train_seq.shape)\n",
    "X_val_seq_scaled = X_val_seq_scaled.reshape(X_val_seq.shape)\n",
    "\n",
    "print(\"Features normalized for LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model 1: Simple LSTM\n",
    "print(\"=\"*60)\n",
    "print(\"Training LSTM Model 1: Simple LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_lstm_simple = Sequential([\n",
    "    LSTM(50, input_shape=(SEQUENCE_LENGTH, X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm_simple.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_simple = model_lstm_simple.fit(\n",
    "    X_train_seq_scaled, y_train_seq,\n",
    "    validation_data=(X_val_seq_scaled, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred_lstm_simple = model_lstm_simple.predict(X_train_seq_scaled, verbose=0).flatten()\n",
    "y_val_pred_lstm_simple = model_lstm_simple.predict(X_val_seq_scaled, verbose=0).flatten()\n",
    "\n",
    "lstm_simple_result = {\n",
    "    'model_name': 'LSTM (Simple)',\n",
    "    'train_mae': mean_absolute_error(y_train_seq, y_train_pred_lstm_simple),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train_seq, y_train_pred_lstm_simple)),\n",
    "    'train_r2': r2_score(y_train_seq, y_train_pred_lstm_simple),\n",
    "    'val_mae': mean_absolute_error(y_val_seq, y_val_pred_lstm_simple),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val_seq, y_val_pred_lstm_simple)),\n",
    "    'val_r2': r2_score(y_val_seq, y_val_pred_lstm_simple),\n",
    "    'model': model_lstm_simple\n",
    "}\n",
    "\n",
    "print(f\"\\nSimple LSTM Performance:\")\n",
    "print(f\"  Train - MAE: {lstm_simple_result['train_mae']:.4f}, RMSE: {lstm_simple_result['train_rmse']:.4f}, R²: {lstm_simple_result['train_r2']:.4f}\")\n",
    "print(f\"  Val   - MAE: {lstm_simple_result['val_mae']:.4f}, RMSE: {lstm_simple_result['val_rmse']:.4f}, R²: {lstm_simple_result['val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186984e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM Model 2: Stacked LSTM with Dropout\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training LSTM Model 2: Stacked LSTM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_lstm_stacked = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQUENCE_LENGTH, X_train_seq.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm_stacked.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history_stacked = model_lstm_stacked.fit(\n",
    "    X_train_seq_scaled, y_train_seq,\n",
    "    validation_data=(X_val_seq_scaled, y_val_seq),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "y_train_pred_lstm_stacked = model_lstm_stacked.predict(X_train_seq_scaled, verbose=0).flatten()\n",
    "y_val_pred_lstm_stacked = model_lstm_stacked.predict(X_val_seq_scaled, verbose=0).flatten()\n",
    "\n",
    "lstm_stacked_result = {\n",
    "    'model_name': 'LSTM (Stacked)',\n",
    "    'train_mae': mean_absolute_error(y_train_seq, y_train_pred_lstm_stacked),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train_seq, y_train_pred_lstm_stacked)),\n",
    "    'train_r2': r2_score(y_train_seq, y_train_pred_lstm_stacked),\n",
    "    'val_mae': mean_absolute_error(y_val_seq, y_val_pred_lstm_stacked),\n",
    "    'val_rmse': np.sqrt(mean_squared_error(y_val_seq, y_val_pred_lstm_stacked)),\n",
    "    'val_r2': r2_score(y_val_seq, y_val_pred_lstm_stacked),\n",
    "    'model': model_lstm_stacked\n",
    "}\n",
    "\n",
    "print(f\"\\nStacked LSTM Performance:\")\n",
    "print(f\"  Train - MAE: {lstm_stacked_result['train_mae']:.4f}, RMSE: {lstm_stacked_result['train_rmse']:.4f}, R²: {lstm_stacked_result['train_r2']:.4f}\")\n",
    "print(f\"  Val   - MAE: {lstm_stacked_result['val_mae']:.4f}, RMSE: {lstm_stacked_result['val_rmse']:.4f}, R²: {lstm_stacked_result['val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df261a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Simple LSTM\n",
    "axes[0, 0].plot(history_simple.history['loss'], label='Train Loss')\n",
    "axes[0, 0].plot(history_simple.history['val_loss'], label='Val Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss (MSE)')\n",
    "axes[0, 0].set_title('Simple LSTM: Training History (Loss)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history_simple.history['mae'], label='Train MAE')\n",
    "axes[0, 1].plot(history_simple.history['val_mae'], label='Val MAE')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].set_title('Simple LSTM: Training History (MAE)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Stacked LSTM\n",
    "axes[1, 0].plot(history_stacked.history['loss'], label='Train Loss')\n",
    "axes[1, 0].plot(history_stacked.history['val_loss'], label='Val Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss (MSE)')\n",
    "axes[1, 0].set_title('Stacked LSTM: Training History (Loss)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history_stacked.history['mae'], label='Train MAE')\n",
    "axes[1, 1].plot(history_stacked.history['val_mae'], label='Val MAE')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('MAE')\n",
    "axes[1, 1].set_title('Stacked LSTM: Training History (MAE)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b4202",
   "metadata": {},
   "source": [
    "### 6.6 Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef51874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results including LSTM models\n",
    "all_results_final = baseline_results + [xgb_result, lgb_result, lstm_simple_result, lstm_stacked_result]\n",
    "\n",
    "# Create comprehensive results summary\n",
    "results_final = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model_name'],\n",
    "        'Train MAE': r['train_mae'],\n",
    "        'Train RMSE': r['train_rmse'],\n",
    "        'Train R²': r['train_r2'],\n",
    "        'Val MAE': r['val_mae'],\n",
    "        'Val RMSE': r['val_rmse'],\n",
    "        'Val R²': r['val_r2'],\n",
    "        'Overfit (MAE diff)': r['train_mae'] - r['val_mae']\n",
    "    }\n",
    "    for r in all_results_final\n",
    "]).sort_values('Val MAE')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_final.to_string(index=False))\n",
    "\n",
    "# Find overall best model\n",
    "best_model_final = min(all_results_final, key=lambda x: x['val_mae'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best_model_final['model_name']}\")\n",
    "print(f\"Validation MAE: {best_model_final['val_mae']:.4f}\")\n",
    "print(f\"Validation RMSE: {best_model_final['val_rmse']:.4f}\")\n",
    "print(f\"Validation R²: {best_model_final['val_r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Validation MAE Comparison\n",
    "models = results_final['Model'].values\n",
    "val_mae = results_final['Val MAE'].values\n",
    "colors = ['red' if mae == val_mae.min() else 'steelblue' for mae in val_mae]\n",
    "\n",
    "axes[0].barh(models, val_mae, color=colors)\n",
    "axes[0].set_xlabel('Validation MAE (lower is better)')\n",
    "axes[0].set_title('Model Comparison: Validation MAE')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 2. Validation RMSE Comparison\n",
    "val_rmse = results_final['Val RMSE'].values\n",
    "colors_rmse = ['red' if rmse == val_rmse.min() else 'steelblue' for rmse in val_rmse]\n",
    "\n",
    "axes[1].barh(models, val_rmse, color=colors_rmse)\n",
    "axes[1].set_xlabel('Validation RMSE (lower is better)')\n",
    "axes[1].set_title('Model Comparison: Validation RMSE')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Validation R² Comparison\n",
    "val_r2 = results_final['Val R²'].values\n",
    "colors_r2 = ['green' if r2 == val_r2.max() else 'steelblue' for r2 in val_r2]\n",
    "\n",
    "axes[2].barh(models, val_r2, color=colors_r2)\n",
    "axes[2].set_xlabel('Validation R² (higher is better)')\n",
    "axes[2].set_title('Model Comparison: Validation R²')\n",
    "axes[2].invert_yaxis()\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b178f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final best model\n",
    "import joblib\n",
    "\n",
    "print(\"Saving best model and metadata...\")\n",
    "\n",
    "# Save best model (handling different model types)\n",
    "if 'LSTM' in best_model_final['model_name']:\n",
    "    # Save Keras model\n",
    "    best_model_final['model'].save('model_lstm.keras')\n",
    "    print(\"✓ Saved model_lstm.keras\")\n",
    "    # Also save as pickle for compatibility\n",
    "    joblib.dump(best_result['model'], 'model.pkl')\n",
    "    print(\"✓ Saved model.pkl (non-LSTM best)\")\n",
    "else:\n",
    "    joblib.dump(best_model_final['model'], 'model.pkl')\n",
    "    print(\"✓ Saved model.pkl\")\n",
    "\n",
    "# Save feature columns\n",
    "joblib.dump(feature_cols_all, 'feature_columns.pkl')\n",
    "print(\"✓ Saved feature_columns.pkl\")\n",
    "\n",
    "# Save sensors to exclude\n",
    "joblib.dump(sensors_to_exclude, 'sensors_to_exclude.pkl')\n",
    "print(\"✓ Saved sensors_to_exclude.pkl\")\n",
    "\n",
    "# Save comprehensive results\n",
    "results_final.to_csv('model_results_comprehensive.csv', index=False)\n",
    "print(\"✓ Saved model_results_comprehensive.csv\")\n",
    "\n",
    "# Save scaler for LSTM\n",
    "if 'LSTM' in best_model_final['model_name']:\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "    print(\"✓ Saved scaler.pkl (for LSTM)\")\n",
    "\n",
    "print(\"\\nAll model artifacts saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
